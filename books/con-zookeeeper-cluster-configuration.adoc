// Module included in the following assemblies:
//
// assembly-configuring-zookeeper.adoc

[id='con-zookeeeper-cluster-configuration-{context}']

= ZooKeeper cluster configuration

For reliable ZooKeeper service, you should deploy ZooKeeper in a cluster.
Hence, for production use cases, you must run a cluster of replicated ZooKeeper instances.
ZooKeeper clusters are also referred as ensembles.

ZooKeeper clusters usually consist of an odd number of nodes.
ZooKeeper requires a majority of the nodes in the cluster should be up and running.
For example, a cluster with three nodes, at least two of them must be up and running.
This means it can tolerate one node being down.
A cluster consisting of five nodes, at least three nodes must be available.
This means it can tolerate two nodes being down.
A cluster consisting of seven nodes, at least four nodes must be available.
This means it can tolerate three nodes being down.
Having more nodes in the ZooKeeper cluster delivers better resiliency and reliability of the whole cluster.

ZooKeeper can run in clusters with an even number of nodes. 
The additional node, however, does not increase the resiliency of the cluster. 
A cluster with four nodes requires at least three nodes to be available and can tolerate only one node being down.
Therefore it has exactly the same resiliency as a cluster with only three nodes.

The different ZooKeeper nodes should be ideally placed into different data centers or network segments.
Increasing the number of ZooKeeper nodes increases the workload spent on cluster synchronization. 
For most Kafka use cases ZooKeeper cluster with 3, 5 or 7 nodes should be fully sufficient.

WARNING: ZooKeeper cluster with 3 nodes can tolerate only 1 unavailable node. 
This means that when a cluster node crashes while you are doing maintenance on another node your ZooKeeper cluster will be unavailable.

Replicated ZooKeeper configuration supports all configuration options supported by the standalone configuration.
Additional options are added for the clustering configuration:

`initLimit`:: Amount of time to allow followers to connect and sync to the cluster leader. 
The time is specified as a number of ticks (see the xref:con-zookeeper-basic-configuration-{context}[`timeTick` option] for more details).
`syncLimit`:: Amount of time for which followers can be behind the leader.
The time is specified as a number of ticks (see the xref:con-zookeeper-basic-configuration-{context}[`timeTick` option] for more details).

In addition to the options above, every configuration file should contain a list of servers which should be members of the ZooKeeper cluster. 
The server records should be specified in the format `server.id=hostname:port1:port2`, where:

`id`:: The ID of the ZooKeeper cluster node.
`hostname`:: The hostname or IP address where the node listens for connections.
`port1`:: The port number used for intra-cluster communication.
`port2`:: The port number used for leader election.

The following is an example configuration file of a ZooKeeper cluster with three nodes:

[source,ini]
----
timeTick=2000
dataDir=/var/lib/zookeeper/
clientPort=2181
initLimit=5
syncLimit=2

server.1=172.17.0.1:2888:3888
server.2=172.17.0.2:2888:3888
server.3=172.17.0.3:2888:3888
----

Each node in the ZooKeeper cluster has to be assigned with a unique `ID`.
Each nodeâ€™s `ID` has to be configured in `myid` file and stored in the `dataDir` folder like `/var/lib/zookeeper/`.
The `myid` files should contain only a single line with the written `ID` as text. 
The `ID` can be any integer from 1 to 255.
You must manually create this file on each cluster node.
Using this file, each ZooKeeper instance will use the configuration from the corresponding `server.` line in the configuration file to configure its listeners.
It will also use all other `server.` lines to identify other cluster members.

In the above example, there are three nodes, so each one will have a different `myid` with values `1`, `2`, and `3` respectively.
